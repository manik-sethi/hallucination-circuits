{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dtype = torch.bfloat16 if device.type == \"cuda\" else torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg, _ = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-it-res-jb\",\n",
    "    sae_id=\"blocks.12.hook_resid_post\",\n",
    "    device = \"cpu\"\n",
    ")\n",
    "sae = sae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = 'blocks.12.hook_resid_post'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Hooked Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"google/gemma-2b-it\", device=device, dtype=dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7b160ac57a24516bccadde9e2745198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/34.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ba24a45190478594c064d8a0f823ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8670a32d24d4d4681465bc99b52e449",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7b21b25b1044b388bcdff37172ad53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"input\"],\n",
    "        truncation=True,\n",
    "        max_length=256,\n",
    "        padding=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id = getattr(model.tokenizer, \"eos_token_id\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = load_dataset(\"facebook/kilt_tasks\", \"eli5\", split=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tok = dataset.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns = [\"id\", \"meta\"],\n",
    "    desc=\"Tokenizing prompts\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def seq2vec(text: str):\n",
    "    toks = model.to_tokens(text, prepend_bos=True).to(device)\n",
    "    _, cache = model.run_with_cache(toks, names_filter=hook)\n",
    "    acts = cache[hook].to(sae.device, dtype=sae.dtype).detach()\n",
    "    feats = sae.encode(acts)\n",
    "    v = feats.mean(dim=1)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode\n",
    "def add_sae(batch):\n",
    "    feats = []\n",
    "    for ans in batch[\"answer_text\"]:\n",
    "        if not ans:\n",
    "            feats.append(None)\n",
    "        v = seq2vec(ans)\n",
    "        feats.append(v.squeeze(0).to(torch.float16).cpu().tolist())\n",
    "    return {\"sae_acts\": feats}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Extract a clean answer_text column\n",
    "def extract_answer_text(batch):\n",
    "    texts = []\n",
    "    for o in batch[\"output\"]:\n",
    "        s = None\n",
    "        if isinstance(o, list) and o:\n",
    "            if isinstance(o[0], dict):\n",
    "                s = o[0].get(\"answer\") or o[0].get(\"text\") or \"\"\n",
    "            elif isinstance(o[0], str):\n",
    "                s = o[0]\n",
    "        elif isinstance(o, dict):\n",
    "            s = o.get(\"answer\") or o.get(\"text\") or \"\"\n",
    "        elif isinstance(o, str):\n",
    "            s = o\n",
    "        texts.append((s or \"\").strip())\n",
    "    return {\"answer_text\": texts}\n",
    "\n",
    "ds_norm = ds_tok.map(extract_answer_text, batched=True, desc=\"Extract answer text\")\n",
    "print(ds_norm[0][\"answer_text\"])  # sanity check: should be a plain str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sae = ds_norm.map(\n",
    "    add_sae,\n",
    "    batched=True,\n",
    "    batch_size=512,\n",
    "    desc=\"Creating SAE dataset!\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
