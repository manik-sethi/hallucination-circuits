{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load query2sae Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo = \"mksethi/gemma-query2sae\"\n",
    "cfg = AutoConfig.from_pretrained(repo, trust_remote_code=True)\n",
    "query2sae = AutoModel.from_pretrained(repo, trust_remote_code=True)\n",
    "\n",
    "print(type(cfg))    # -> Query2SAEConfig\n",
    "print(type(query2sae))  # -> Query2SAEModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query2sae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side=\"right\"\n",
    "tokenizer.truncation_side=\"right\"\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "gemma.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load truthfulQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"domenicrosati/TruthfulQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.remove_columns(['Type', 'Category', 'Source'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in SAE tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sae, cfg, _ = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-it-res-jb\",\n",
    "    sae_id=\"blocks.12.hook_resid_post\",\n",
    "    device = \"cpu\"\n",
    ")\n",
    "sae = sae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\"google/gemma-2b-it\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "def sae_fn(batch):\n",
    "    correct_ans = batch['correct_answer']\n",
    "    inc_ans = batch['incorrect_answer']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        _, cache = model.run_with_cache(correct_ans)\n",
    "        resid = cache['blocks.12.hook_resid_post']\n",
    "        flattened = resid.reshape(-1, resid.shape[-1])\n",
    "        c_sae_features = sae.encode(flattened)\n",
    "        c_sae_reshaped = c_sae_features.reshape(\n",
    "            len(correct_ans), \n",
    "            resid.shape[1], \n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        _, cache = model.run_with_cache(inc_ans)\n",
    "        resid = cache['blocks.12.hook_resid_post']\n",
    "        flattened = resid.reshape(-1, resid.shape[-1])\n",
    "        i_sae_features = sae.encode(flattened)\n",
    "        i_sae_reshaped = i_sae_features.reshape(\n",
    "            len(inc_ans), \n",
    "            resid.shape[1], \n",
    "            -1\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'correct_sae_features': c_sae_reshaped,\n",
    "        'incorrect_sae_features': i_sae_reshaped\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_fn(batch):\n",
    "    questions = batch['Question']\n",
    "    correct_ans = batch['Best Answer']\n",
    "    inc_ans = batch['Incorrect Answers']\n",
    "\n",
    "    q_out = tokenizer(\n",
    "        questions,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    c_out = tokenizer(\n",
    "        correct_ans,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    i_out = tokenizer(\n",
    "        correct_ans,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    return {\n",
    "        'question': q_out['input_ids'], 'q_attention_mask': q_out['attention_mask'],\n",
    "        'correct_answer': c_out['input_ids'], 'c_attention_mask': c_out['attention_mask'],\n",
    "        'incorrect_answer': i_out['input_ids'], 'i_attention_mask': i_out['attention_mask'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tok = ds.map(\n",
    "    tok_fn,\n",
    "    batched=True,\n",
    "    remove_columns=['Type', 'Category', 'Question', 'Best Answer', 'Correct Answers', 'Incorrect Answers', 'Source'],\n",
    "    desc=\"Tokenizing Questions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sae = ds_tok.map(\n",
    "    sae_fn,\n",
    "    batched=True,\n",
    "    desc=\"Getting Answer SAE's\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
