{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from sae_lens import SAE\n",
    "from transformer_lens import HookedTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x76f95895a790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load query2sae Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers_modules.mksethi.gemma-query2sae.b722da8493f29c204bce3a980e816d6ce939def1.configuration_query2sae.Query2SAEConfig'>\n",
      "<class 'transformers_modules.mksethi.gemma-query2sae.b722da8493f29c204bce3a980e816d6ce939def1.modeling_query2sae.Query2SAEModel'>\n"
     ]
    }
   ],
   "source": [
    "repo = \"mksethi/gemma-query2sae\"\n",
    "cfg = AutoConfig.from_pretrained(repo, trust_remote_code=True)\n",
    "query2sae = AutoModel.from_pretrained(repo, trust_remote_code=True)\n",
    "\n",
    "print(type(cfg))    # -> Query2SAEConfig\n",
    "print(type(query2sae))  # -> Query2SAEModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Query2SAEModel(\n",
       "  (backbone): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=128, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=128, out_features=16384, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query2sae.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load gemma-2b-it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394cfe64f1d34cb2b0a9dcdfde86bdb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): GELUActivation()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side=\"right\"\n",
    "tokenizer.truncation_side=\"right\"\n",
    "gemma = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b-it\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "gemma.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load truthfulQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"domenicrosati/TruthfulQA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Type', 'Category', 'Question', 'Best Answer', 'Correct Answers', 'Incorrect Answers', 'Source'],\n",
       "        num_rows: 817\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ds = ds.remove_columns(['Type', 'Category', 'Source'])\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in SAE tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sae_lens/saes/sae.py:249: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_63850/1636573835.py:1: DeprecationWarning: Unpacking SAE objects is deprecated. SAE.from_pretrained() now returns only the SAE object. Use SAE.from_pretrained_with_cfg_and_sparsity() to get the config dict and sparsity as well.\n",
      "  sae, cfg, _ = SAE.from_pretrained(\n"
     ]
    }
   ],
   "source": [
    "sae, cfg, _ = SAE.from_pretrained(\n",
    "    release=\"gemma-2b-it-res-jb\",\n",
    "    sae_id=\"blocks.12.hook_resid_post\",\n",
    "    device = \"cpu\"\n",
    ")\n",
    "sae = sae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf226aaaf2c422ea75d333566a6f253",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:With reduced precision, it is advised to use `from_pretrained_no_processing` instead of `from_pretrained`.\n",
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"google/gemma-2b-it\", torch_dtype=torch.bfloat16).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "def sae_fn(batch):\n",
    "    correct_ans = batch['Correct Answers']\n",
    "    inc_ans = batch['Incorrect Answers']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        _, cache = model.run_with_cache(correct_ans)\n",
    "        resid = cache['blocks.12.hook_resid_post']\n",
    "        flattened = resid.reshape(-1, resid.shape[-1])\n",
    "        c_sae_features = sae.encode(flattened)\n",
    "        c_sae_reshaped = c_sae_features.reshape(\n",
    "            len(correct_ans), \n",
    "            resid.shape[1], \n",
    "            -1\n",
    "        )\n",
    "        c_sae_summed = torch.sum(c_sae_reshaped, dim=1)\n",
    "        \n",
    "        _, cache = model.run_with_cache(inc_ans)\n",
    "        resid = cache['blocks.12.hook_resid_post']\n",
    "        flattened = resid.reshape(-1, resid.shape[-1])\n",
    "        i_sae_features = sae.encode(flattened)\n",
    "        i_sae_reshaped = i_sae_features.reshape(\n",
    "            len(inc_ans), \n",
    "            resid.shape[1], \n",
    "            -1\n",
    "        )\n",
    "        i_sae_summed = torch.sum(i_sae_reshaped, dim=1)\n",
    "\n",
    "    return {\n",
    "        'correct_sae_features': c_sae_summed.cpu().numpy(),\n",
    "        'incorrect_sae_features': i_sae_summed.cpu().numpy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_fn(batch):\n",
    "    questions = batch['Question']\n",
    "    correct_ans = batch['Best Answer']\n",
    "    inc_ans = batch['Incorrect Answers']\n",
    "\n",
    "    q_out = tokenizer(\n",
    "        questions,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    c_out = tokenizer(\n",
    "        correct_ans,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "\n",
    "    i_out = tokenizer(\n",
    "        correct_ans,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=256\n",
    "    )\n",
    "    return {\n",
    "        'question': q_out['input_ids'], 'q_attention_mask': q_out['attention_mask'],\n",
    "        'correct_answer': c_out['input_ids'], 'c_attention_mask': c_out['attention_mask'],\n",
    "        'incorrect_answer': i_out['input_ids'], 'i_attention_mask': i_out['attention_mask'],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_tok = ds.map(\n",
    "    tok_fn,\n",
    "    batched=True,\n",
    "    # remove_columns=['Type', 'Category', 'Question', 'Best Answer', 'Correct Answers', 'Incorrect Answers', 'Source'],\n",
    "    desc=\"Tokenizing Questions\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Type', 'Category', 'Question', 'Best Answer', 'Correct Answers', 'Incorrect Answers', 'Source', 'question', 'q_attention_mask', 'correct_answer', 'c_attention_mask', 'incorrect_answer', 'i_attention_mask'],\n",
       "        num_rows: 817\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_sae = ds_tok.map(\n",
    "    sae_fn,\n",
    "    batched=True,\n",
    "    batch_size=8,\n",
    "    desc=\"Getting Answer SAE's\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_sae = torch.tensor(ds_sae['train']['correct_sae_features'])\n",
    "i_sae = torch.tensor(ds_sae['train']['incorrect_sae_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "c_sae_norm = F.normalize(c_sae, p=2, dim=1)\n",
    "i_sae_norm = F.normalize(i_sae, p=2, dim=1)\n",
    "pairwise_similarity = torch.matmul(i_sae_norm, c_sae_norm.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9608, 0.8893, 0.6497,  ..., 0.8904, 0.8170, 0.5271],\n",
       "        [0.9340, 0.9645, 0.6692,  ..., 0.8960, 0.8360, 0.5733],\n",
       "        [0.8741, 0.8406, 0.8961,  ..., 0.8702, 0.8341, 0.6364],\n",
       "        ...,\n",
       "        [0.9008, 0.8290, 0.5550,  ..., 0.8168, 0.7068, 0.3710],\n",
       "        [0.9048, 0.8381, 0.5664,  ..., 0.8128, 0.7356, 0.3976],\n",
       "        [0.6389, 0.6802, 0.6861,  ..., 0.7997, 0.8682, 0.9327]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "diag = torch.diag(pairwise_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8947)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(diag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the Jaccard similarity tensor: torch.Size([817])\n",
      "First 10 Jaccard similarity scores: tensor([0.3636, 0.5702, 0.5756, 0.6255, 0.4551, 0.6518, 0.4489, 0.4991, 0.5514,\n",
      "        0.5466])\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Create boolean masks for the non-zero elements ---\n",
    "# This will result in tensors of shape (817, 16384) with True where the element is non-zero\n",
    "c_sae_mask = c_sae != 0\n",
    "i_sae_mask = i_sae != 0\n",
    "\n",
    "# --- Step 2: Calculate the Intersection and Union Sizes ---\n",
    "intersection_size = (c_sae_mask & i_sae_mask).sum(dim=1)\n",
    "\n",
    "union_size = (c_sae_mask | i_sae_mask).sum(dim=1)\n",
    "\n",
    "# --- Step 3: Calculate the Jaccard Similarity ---\n",
    "jaccard_similarity = intersection_size.float() / union_size.float()\n",
    "\n",
    "# --- Step 4: Handle the edge case of all-zero vectors ---\n",
    "\n",
    "jaccard_similarity[union_size == 0] = 0.0\n",
    "\n",
    "print(\"Shape of the Jaccard similarity tensor:\", jaccard_similarity.shape)\n",
    "print(\"First 10 Jaccard similarity scores:\", jaccard_similarity[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.4476)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(jaccard_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete!\n",
      "Output: {'logits': tensor([[ 0.0050, -0.0026, -0.0013,  ...,  0.3934, -0.0087,  0.0142]],\n",
      "       device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "gpt2_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "if gpt2_tokenizer.pad_token is None:\n",
    "    gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "\n",
    "# Your question\n",
    "query = \"What happens to you if you eat watermelon seeds?\"\n",
    "\n",
    "# Get model device\n",
    "device = next(query2sae.parameters()).device\n",
    "\n",
    "# Tokenize and move to device\n",
    "tokenized_input = gpt2_tokenizer(\n",
    "    query,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=256\n",
    ")\n",
    "tokenized_input = {k: v.to(device) for k, v in tokenized_input.items()}\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = query2sae(**tokenized_input)\n",
    "\n",
    "print(\"Inference complete!\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16384])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['logits'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def jaccard_similarity_vectors(tensor_a: torch.Tensor, tensor_b: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Computes the Jaccard similarity between two 1D PyTorch tensors.\n",
    "\n",
    "    The Jaccard similarity is defined as the size of the intersection divided\n",
    "    by the size of the union of the non-zero elements.\n",
    "\n",
    "    Args:\n",
    "        tensor_a (torch.Tensor): A tensor of shape (1, 16384) representing the first vector.\n",
    "        tensor_b (torch.Tensor): A tensor of shape (1, 16384) representing the second vector.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A scalar tensor containing the Jaccard similarity score.\n",
    "    \"\"\"\n",
    "    # Ensure the tensors are 1D for simplicity\n",
    "    tensor_a = tensor_a.squeeze()\n",
    "    tensor_b = tensor_b.squeeze()\n",
    "    \n",
    "    # Create boolean masks for the non-zero elements\n",
    "    mask_a = tensor_a != 0\n",
    "    mask_b = tensor_b != 0\n",
    "\n",
    "    # Calculate the size of the intersection (where both are True)\n",
    "    intersection_size = (mask_a & mask_b).sum()\n",
    "\n",
    "    # Calculate the size of the union (where at least one is True)\n",
    "    union_size = (mask_a | mask_b).sum()\n",
    "\n",
    "    # Handle the edge case where both vectors are all zeros\n",
    "    if union_size == 0:\n",
    "        return torch.tensor(0.0, device=tensor_a.device)\n",
    "    \n",
    "    # Calculate Jaccard similarity\n",
    "    jaccard_score = intersection_size.float() / union_size.float()\n",
    "    \n",
    "    return jaccard_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0778, device='cuda:0')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logits_gpu = output['logits'].to(device)\n",
    "i_sae_0_gpu = i_sae[1].to(device)\n",
    "\n",
    "# 2. THEN, call the function with the GPU tensors\n",
    "jaccard_similarity_vectors(output_logits_gpu, i_sae_0_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What happens to you if you eat watermelon seeds?\n",
      "Where did fortune cookies originate?\n",
      "Why do veins appear blue?\n",
      "What is the spiciest part of a chili pepper?\n",
      "How long should you wait before filing a missing person report?\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "\n",
    "    x = ds['train'][i]['Question']\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF = []\n",
    "correct = []\n",
    "for i in range(817):\n",
    "\n",
    "    query = ds['train'][i]['Question']\n",
    "    device = next(query2sae.parameters()).device\n",
    "\n",
    "    # Tokenize and move to device\n",
    "    tokenized_input = gpt2_tokenizer(\n",
    "        query,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=256\n",
    "    )\n",
    "    tokenized_input = {k: v.to(device) for k, v in tokenized_input.items()}\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        output = query2sae(**tokenized_input)\n",
    "\n",
    "\n",
    "    score = jaccard_similarity_vectors(output['logits'],i_sae[i].to(device))\n",
    "    SF.append(score)\n",
    "\n",
    "    sim = jaccard_similarity_vectors(output['logits'], output['logits'])\n",
    "    correct.append(sim)\n",
    "    \n",
    "f_scores = torch.tensor(SF).to(device).detach()\n",
    "sim_scores = torch.tensor(correct).to(device).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0714, device='cuda:0')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(f_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.mean(sim_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model from: mksethi/gemma-query2sae\n",
      "Loading tokenizer from: gpt2\n",
      "Model and tokenizer loaded successfully.\n",
      "\n",
      "--- Running Consistency Test ---\n",
      "Query 1: 'What happens if you eat watermelon seeds?'\n",
      "Jaccard similarity between two runs of the same query: 1.0000\n",
      "\n",
      "--- Running Sensitivity Test ---\n",
      "Query 2: 'What is the capital of France?'\n",
      "Jaccard similarity between Query 1 and Query 2: 1.0000\n",
      "\n",
      "--- Running Sparsity Test ---\n",
      "Number of non-zero features for Query 1: 16384 / 16384\n",
      "Number of non-zero features for Query 2: 16384 / 16384\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# --- Utility Function for Jaccard Similarity ---\n",
    "def jaccard_similarity_vectors(tensor_a, tensor_b):\n",
    "    \"\"\"Computes the Jaccard similarity between two 1D PyTorch tensors.\"\"\"\n",
    "    tensor_a = tensor_a.squeeze()\n",
    "    tensor_b = tensor_b.squeeze()\n",
    "    \n",
    "    mask_a = tensor_a != 0\n",
    "    mask_b = tensor_b != 0\n",
    "\n",
    "    intersection_size = (mask_a & mask_b).sum().float()\n",
    "    union_size = (mask_a | mask_b).sum().float()\n",
    "\n",
    "    if union_size == 0:\n",
    "        return torch.tensor(0.0, device=tensor_a.device)\n",
    "    \n",
    "    return intersection_size / union_size\n",
    "\n",
    "# --- 1. Setup and Model Loading ---\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"mksethi/gemma-query2sae\"\n",
    "tokenizer_path = \"gpt2\"  # Using GPT-2 tokenizer, as per your training\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the GPT-2 tokenizer, which is the correct one for your trained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n",
    "    query2sae = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device)\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 2. Define the queries to test ---\n",
    "query1 = \"What happens if you eat watermelon seeds?\"\n",
    "query2 = \"What is the capital of France?\"\n",
    "\n",
    "# --- 3. Consistency Test (Same input, should be identical output) ---\n",
    "print(\"\\n--- Running Consistency Test ---\")\n",
    "print(f\"Query 1: '{query1}'\")\n",
    "\n",
    "# Run the model multiple times with the same input\n",
    "with torch.no_grad():\n",
    "    tokenized_input1_run1 = {k: v.to(device) for k, v in tokenizer(query1, return_tensors=\"pt\").items()}\n",
    "    output1_run1 = query2sae(**tokenized_input1_run1)['logits']\n",
    "\n",
    "    tokenized_input1_run2 = {k: v.to(device) for k, v in tokenizer(query1, return_tensors=\"pt\").items()}\n",
    "    output1_run2 = query2sae(**tokenized_input1_run2)['logits']\n",
    "\n",
    "    # Calculate Jaccard similarity between the two runs\n",
    "    similarity_score_consistency = jaccard_similarity_vectors(output1_run1, output1_run2)\n",
    "\n",
    "print(f\"Jaccard similarity between two runs of the same query: {similarity_score_consistency.item():.4f}\")\n",
    "\n",
    "# --- 4. Sensitivity Test (Different inputs, should be different outputs) ---\n",
    "print(\"\\n--- Running Sensitivity Test ---\")\n",
    "print(f\"Query 2: '{query2}'\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    tokenized_input2 = {k: v.to(device) for k, v in tokenizer(query2, return_tensors=\"pt\").items()}\n",
    "    output2 = query2sae(**tokenized_input2)['logits']\n",
    "\n",
    "    # Calculate Jaccard similarity between the two different queries\n",
    "    similarity_score_sensitivity = jaccard_similarity_vectors(output1_run1, output2)\n",
    "\n",
    "print(f\"Jaccard similarity between Query 1 and Query 2: {similarity_score_sensitivity.item():.4f}\")\n",
    "\n",
    "# --- 5. Sparsity Test (Check the number of non-zero features) ---\n",
    "print(\"\\n--- Running Sparsity Test ---\")\n",
    "num_non_zero_1 = (output1_run1.squeeze() != 0).sum().item()\n",
    "num_non_zero_2 = (output2.squeeze() != 0).sum().item()\n",
    "\n",
    "print(f\"Number of non-zero features for Query 1: {num_non_zero_1} / {output1_run1.shape[1]}\")\n",
    "print(f\"Number of non-zero features for Query 2: {num_non_zero_2} / {output2.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading model from: mksethi/gemma-query2sae\n",
      "Loading tokenizer from: gpt2\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7bf3f8fbc449149911fd2333d512ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Loaded Model Parameters to Randomly Initialized Model ---\n",
      "\n",
      "Parameter: head.0.weight\n",
      "  Loaded Model: Mean=-0.000710, Std=0.018914, Norm=5.934415\n",
      "  Random Model: Mean=-0.000097, Std=0.020839, Norm=6.533765\n",
      "  Absolute Difference Stats: Mean=0.022506, Max=0.237118\n",
      "  Loaded and Random Tensors are identical: False\n",
      "\n",
      "Parameter: head.0.bias\n",
      "  Loaded Model: Mean=-0.017200, Std=0.026107, Norm=0.352742\n",
      "  Random Model: Mean=0.001677, Std=0.020560, Norm=0.232479\n",
      "  Absolute Difference Stats: Mean=0.031042, Max=0.095810\n",
      "  Loaded and Random Tensors are identical: False\n",
      "\n",
      "Parameter: head.2.weight\n",
      "  Loaded Model: Mean=0.000066, Std=0.047345, Norm=68.561699\n",
      "  Random Model: Mean=0.000020, Std=0.051026, Norm=73.891800\n",
      "  Absolute Difference Stats: Mean=0.056838, Max=0.481538\n",
      "  Loaded and Random Tensors are identical: False\n",
      "\n",
      "Parameter: head.2.bias\n",
      "  Loaded Model: Mean=0.002605, Std=0.009792, Norm=1.296951\n",
      "  Random Model: Mean=-0.000146, Std=0.051031, Norm=6.531801\n",
      "  Absolute Difference Stats: Mean=0.044713, Max=0.198551\n",
      "  Loaded and Random Tensors are identical: False\n",
      "\n",
      "--- Analysis Complete ---\n",
      "If the parameters are identical, the loaded model's weights have not been updated.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, AutoModel, AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# --- 1. Define the Query2SAE model class from your training script ---\n",
    "# This is necessary to create a freshly initialized, random version for comparison.\n",
    "class Query2SAE(nn.Module):\n",
    "    def __init__(self, head_hidden_dim: int, sae_dim: int):\n",
    "        super().__init__()\n",
    "        self.backbone = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False  # freeze GPT-2\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(self.backbone.config.hidden_size, head_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(head_hidden_dim, sae_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        with torch.no_grad():\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            last_hidden = out.last_hidden_state[:, -1, :]\n",
    "        return self.head(last_hidden)\n",
    "\n",
    "# --- 2. Setup and Model Loading ---\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"mksethi/gemma-query2sae\"\n",
    "tokenizer_path = \"gpt2\"\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
    "\n",
    "try:\n",
    "    # Load the GPT-2 tokenizer and the trained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n",
    "    loaded_model = AutoModel.from_pretrained(model_path, trust_remote_code=True).to(device)\n",
    "    print(\"Model and tokenizer loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# --- 3. Instantiate a fresh, randomly initialized model ---\n",
    "# We get the SAE dimension from the loaded model's final linear layer.\n",
    "# We'll use a default head dimension for the random model.\n",
    "sae_dim = loaded_model.head[2].out_features\n",
    "head_dim = loaded_model.head[0].out_features\n",
    "random_model = Query2SAE(head_hidden_dim=head_dim, sae_dim=sae_dim)\n",
    "\n",
    "print(\"\\n--- Comparing Loaded Model Parameters to Randomly Initialized Model ---\")\n",
    "\n",
    "# --- 4. Iterate and compare the parameters ---\n",
    "loaded_params = loaded_model.state_dict()\n",
    "random_params = random_model.state_dict()\n",
    "\n",
    "for name in loaded_params:\n",
    "    # We are only interested in the trainable parameters of the head\n",
    "    if \"head\" in name:\n",
    "        loaded_tensor = loaded_params[name].cpu()\n",
    "        random_tensor = random_params[name].cpu()\n",
    "\n",
    "        # Calculate key statistics for the loaded model's parameters\n",
    "        loaded_mean = loaded_tensor.mean().item()\n",
    "        loaded_std = loaded_tensor.std().item()\n",
    "        loaded_norm = torch.norm(loaded_tensor).item()\n",
    "\n",
    "        # Calculate key statistics for the random model's parameters\n",
    "        random_mean = random_tensor.mean().item()\n",
    "        random_std = random_tensor.std().item()\n",
    "        random_norm = torch.norm(random_tensor).item()\n",
    "\n",
    "        # Calculate the difference and its stats\n",
    "        diff_tensor = loaded_tensor - random_tensor\n",
    "        diff_mean = diff_tensor.abs().mean().item()\n",
    "        diff_max = diff_tensor.abs().max().item()\n",
    "\n",
    "        print(f\"\\nParameter: {name}\")\n",
    "        print(f\"  Loaded Model: Mean={loaded_mean:.6f}, Std={loaded_std:.6f}, Norm={loaded_norm:.6f}\")\n",
    "        print(f\"  Random Model: Mean={random_mean:.6f}, Std={random_std:.6f}, Norm={random_norm:.6f}\")\n",
    "        print(f\"  Absolute Difference Stats: Mean={diff_mean:.6f}, Max={diff_max:.6f}\")\n",
    "\n",
    "        # The core check: Are the tensors identical?\n",
    "        is_identical = torch.allclose(loaded_tensor, random_tensor)\n",
    "        print(f\"  Loaded and Random Tensors are identical: {is_identical}\")\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")\n",
    "print(\"If the parameters are identical, the loaded model's weights have not been updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
