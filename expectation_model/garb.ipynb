{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Query2SAE → head_hidden_dim=128, sae_dim=24576 on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from head import Query2SAE, get_hyperparams\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# load the same hyperparams you trained with\n",
    "_, _, _, _, head_dim, _ = get_hyperparams()\n",
    "\n",
    "# load your safetensors checkpoint\n",
    "state_dict = load_file(\"checkpoint/model_epoch4.safetensors\")\n",
    "# determine SAE dimension from one of the weight tensors\n",
    "sae_dim = state_dict[\"head.2.weight\"].shape[0]\n",
    "\n",
    "# instantiate & load weights\n",
    "model = Query2SAE(head_hidden_dim=head_dim, sae_dim=sae_dim)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Loaded Query2SAE → head_hidden_dim={head_dim}, sae_dim={sae_dim} on {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer setup complete. Pad token: '[PAD]'\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# FIX: Add proper pad token instead of using eos_token\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add new pad token\n",
    "\n",
    "lm = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resize embeddings for new pad token\n",
    "lm.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move to device and set pad token ID\n",
    "lm = lm.to(device)\n",
    "lm.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Tokenizer setup complete. Pad token: '{tokenizer.pad_token}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 50])\n",
      "Q: How can I teleport using only kitchen appliances?\n",
      "A: How can I teleport using only kitchen appliances?\n",
      "You could use a regular electric stove to put your food into the microwave, or you might make it by putting an old-fashioned gas cooker in there. But if that doesn't work out for you, just try cooking with other ingredients—and sometimes even some vinegar and mustard!\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I teleport using only kitchen appliances?\"\n",
    "\n",
    "# FIX: Use shorter max_length to avoid memory issues and proper attention mask handling\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=50,  # REDUCED from 100\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():  # FIX: Add memory management\n",
    "        gen_ids = lm.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=512,  # REDUCED from 256\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,  # FIX: Add to reduce repetition\n",
    "            no_repeat_ngram_size=2   # FIX: Avoid repeating phrases\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"A:\", answer)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del gen_ids\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"GPU memory issue - trying CPU fallback...\")\n",
    "        # Move to CPU and retry\n",
    "        lm_cpu = lm.to('cpu')\n",
    "        inputs_cpu = {k: v.to('cpu') for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen_ids = lm_cpu.generate(\n",
    "                inputs_cpu[\"input_ids\"],\n",
    "                attention_mask=inputs_cpu[\"attention_mask\"],\n",
    "                max_length=100,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        answer = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        print(\"Q:\", question)\n",
    "        print(\"A (CPU):\", answer)\n",
    "    else:\n",
    "        print(f\"Generation error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual LM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You could use a regular electric stove to put your food into the microwave, or you might make it by putting an old-fashioned gas cooker in there. But if that doesn't work out for you, just try cooking with other ingredients—and sometimes even some vinegar and mustard!\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.split(sep=\"\\n\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicted Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from head import Query2SAE, get_hyperparams\n",
    "from safetensors.torch import load_file\n",
    "import torch\n",
    "\n",
    "# 1) load checkpoint\n",
    "# state_dict = load_file(\"checkpoints/model_epoch4.safetensors\")\n",
    "state_dit = load_file(\"./checkpoint/model_epoch4.safetensors\")\n",
    "sae_dim    = state_dict[\"head.2.weight\"].shape[0]\n",
    "_, _, _, _, head_dim, _ = get_hyperparams()\n",
    "\n",
    "model = Query2SAE(head_hidden_dim=head_dim, sae_dim=sae_dim)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 2) instantiate GPT-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # gpt2 has no pad token by default\n",
    "\n",
    "# 3) tokenize *your question*, not your SAE features!\n",
    "question = \"How can I teleport using only kitchen appliances?\"\n",
    "inputs   = tokenizer(\n",
    "    question,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256\n",
    ").to(device)\n",
    "\n",
    "# 4) run through your Query2SAE\n",
    "with torch.no_grad():\n",
    "    pred_sae = model(\n",
    "        inputs[\"input_ids\"], \n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )  # shape (1, sae_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24576])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_sae.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## albert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = Variant(\n",
    "    model_id=\"gpt2-small\",\n",
    "    sae_release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.11.hook_resid_pre\",\n",
    ")\n",
    "model, sae, cfg, tokenizer = gpt.get_components()\n",
    "sae.eval()\n",
    "features = sae.encode(cache[sae.cfg.hook_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using HookedSAETransformer\n",
      "Loaded pretrained model gpt2-small into HookedTransformer\n",
      "Moving model to device:  cpu\n",
      "SAE vector shape: torch.Size([1, 24576])\n",
      "Top-50 SAE feature indices: [8100, 21800, 24098, 14364, 14132, 2049, 23635, 3565, 15055, 17438, 18258, 2910, 21927, 24151, 9576, 22944, 5921, 16477, 21182, 19197, 2227, 7678, 12484, 23028, 21508, 11072, 20176, 4527, 10615, 1652, 2024, 23511, 10089, 19111, 23128, 9788, 13296, 23585, 21065, 19744, 16770, 13837, 11106, 19678, 2426, 21716, 13442, 3130, 7071, 1876]\n"
     ]
    }
   ],
   "source": [
    "# at the top of your notebook…\n",
    "import os, sys\n",
    "import torch\n",
    "\n",
    "# 1) get the directory containing this notebook (…/hallucination-circuits/expectation_model)\n",
    "wd = os.getcwd()\n",
    "# 2) go up one level to …/hallucination-circuits\n",
    "project_root = os.path.abspath(os.path.join(wd, \"..\"))\n",
    "# 3) insert it at the front of sys.path\n",
    "sys.path.insert(0, project_root)\n",
    "\n",
    "# now you can import your Variant helper\n",
    "from src.interfaces.lens_backend import Variant\n",
    "\n",
    "def encode_text_to_sae(text: str, max_length: int = 256):\n",
    "    # 1) Spin up the Variant and grab model / SAE / cfg / tokenizer\n",
    "    gpt = Variant(\n",
    "        model_id=\"gpt2-small\",\n",
    "        sae_release=\"gpt2-small-res-jb\",\n",
    "        sae_id=\"blocks.11.hook_resid_pre\",\n",
    "    )\n",
    "    model, sae, cfg, tokenizer = gpt.get_components()\n",
    "    device = gpt.device\n",
    "    model.to(device)\n",
    "\n",
    "    # 2) Tokenize your text (Lens’s tokenizer) – we only need input_ids\n",
    "    toks = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "    )\n",
    "    tokens = toks[\"input_ids\"].to(device)  # shape [1, seq_len]\n",
    "\n",
    "    # 3) Run the GPT-2 backbone (Lens will stash activations in model.cache)\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens)  # ← pass only the token IDs!\n",
    "\n",
    "    # 4) Pull out the hidden‐state at exactly the layer you trained your SAE on\n",
    "    #    cfg.hook_name is e.g. \"blocks.11.hook_resid_pre\"\n",
    "    hidden_states: torch.Tensor = cache[cfg.hook_name]\n",
    "    #   → shape [batch_size, seq_len, d_model]\n",
    "\n",
    "    # 5) Pool however you trained (here: last token)\n",
    "    last_hidden = hidden_states[:, -1, :]  # → [batch_size, d_model]\n",
    "\n",
    "    # 6) Encode into the full SAE space\n",
    "    with torch.no_grad():\n",
    "        sae_vector = sae.encode(last_hidden)  # → [batch_size, sae_dim]\n",
    "\n",
    "    return sae_vector  # tensor of shape [1, sae_dim]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"Why do flamingos stand on one leg?\"\n",
    "    vec = encode_text_to_sae(query)\n",
    "    print(\"SAE vector shape:\", vec.shape)\n",
    "\n",
    "    # e.g. find top-50 indices:\n",
    "    top50 = torch.topk(vec, k=50, dim=-1).indices[0].tolist()\n",
    "    print(\"Top-50 SAE feature indices:\", top50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(top50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
