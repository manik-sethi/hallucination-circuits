{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Query2SAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Query2SAE → head_hidden_dim=128, sae_dim=24576 on cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from head import Query2SAE, get_hyperparams\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# load the same hyperparams you trained with\n",
    "_, _, _, _, head_dim, _ = get_hyperparams()\n",
    "\n",
    "# load your safetensors checkpoint\n",
    "state_dict = load_file(\"checkpoint/model_epoch4.safetensors\")\n",
    "# determine SAE dimension from one of the weight tensors\n",
    "sae_dim = state_dict[\"head.2.weight\"].shape[0]\n",
    "\n",
    "# instantiate & load weights\n",
    "model = Query2SAE(head_hidden_dim=head_dim, sae_dim=sae_dim)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Loaded Query2SAE → head_hidden_dim={head_dim}, sae_dim={sae_dim} on {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GPT-2 Small Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer setup complete. Pad token: '[PAD]'\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel\n",
    "\n",
    "# FIX: Add proper pad token instead of using eos_token\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add new pad token\n",
    "\n",
    "lm = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Resize embeddings for new pad token\n",
    "lm.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move to device and set pad token ID\n",
    "lm = lm.to(device)\n",
    "lm.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"Tokenizer setup complete. Pad token: '{tokenizer.pad_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate GPT-2 Small Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 50])\n",
      "Q: What is the capital of the US\n",
      "A: What is the capital of the US?\n",
      "The word \"capital\" comes from Latin. The United States has about 100 million people and an estimated $15 trillion in debt, with roughly 20% to 30% of that coming from foreign countries such as China and India. In addition there are some 500 state-owned banks (mostly owned by corporations), most notably those for utilities like solar power or wind farms. Most importantly however, according Toowoomba County's resident economist David Tipton he estimates total federal government spending on health care will be around 13 billion dollars over 10 years—around a fifth of what it was before Obamacare came into effect this year. As you can see here , while I believe healthcare costs continue to climb steadily since last December, even though many Americans have more coverage than they did four months ago, these numbers don't reflect reality: While we're at it, do any other states really need another bailout if their own medical facilities aren' not paying down debts quickly enough so taxpayers won�t suffer through anything else until next summer?I'll leave everything up when answering my question once again; please let me know how your answer turned out!\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the capital of the US\"\n",
    "\n",
    "# FIX: Use shorter max_length to avoid memory issues and proper attention mask handling\n",
    "inputs = tokenizer(\n",
    "    question,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=50,  # REDUCED from 100\n",
    "    truncation=True\n",
    ").to(device)\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():  # FIX: Add memory management\n",
    "        gen_ids = lm.generate(\n",
    "            inputs[\"input_ids\"], \n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_length=512,  # REDUCED from 256\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.1,  # FIX: Add to reduce repetition\n",
    "            no_repeat_ngram_size=2   # FIX: Avoid repeating phrases\n",
    "        )\n",
    "    \n",
    "    answer = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    print(\"Q:\", question)\n",
    "    print(\"A:\", answer)\n",
    "    \n",
    "    # Clean up memory\n",
    "    del gen_ids\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "except RuntimeError as e:\n",
    "    if \"out of memory\" in str(e):\n",
    "        print(\"GPU memory issue - trying CPU fallback...\")\n",
    "        # Move to CPU and retry\n",
    "        lm_cpu = lm.to('cpu')\n",
    "        inputs_cpu = {k: v.to('cpu') for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gen_ids = lm_cpu.generate(\n",
    "                inputs_cpu[\"input_ids\"],\n",
    "                attention_mask=inputs_cpu[\"attention_mask\"],\n",
    "                max_length=100,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                do_sample=True,\n",
    "                temperature=0.7\n",
    "            )\n",
    "        \n",
    "        answer = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        print(\"Q:\", question)\n",
    "        print(\"A (CPU):\", answer)\n",
    "    else:\n",
    "        print(f\"Generation error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual LM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The word \"capital\" comes from Latin. The United States has about 100 million people and an estimated $15 trillion in debt, with roughly 20% to 30% of that coming from foreign countries such as China and India. In addition there are some 500 state-owned banks (mostly owned by corporations), most notably those for utilities like solar power or wind farms. Most importantly however, according Toowoomba County\\'s resident economist David Tipton he estimates total federal government spending on health care will be around 13 billion dollars over 10 years—around a fifth of what it was before Obamacare came into effect this year. As you can see here , while I believe healthcare costs continue to climb steadily since last December, even though many Americans have more coverage than they did four months ago, these numbers don\\'t reflect reality: While we\\'re at it, do any other states really need another bailout if their own medical facilities aren\\' not paying down debts quickly enough so taxpayers won�t suffer through anything else until next summer?I\\'ll leave everything up when answering my question once again; please let me know how your answer turned out!'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.split(sep=\"\\n\")[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate F_Expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from head import Query2SAE, get_hyperparams\n",
    "from safetensors.torch import load_file\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 1) load checkpoint\n",
    "path = os.path.join(os.getcwd(), 'checkpoint', 'model_epoch4.safetensors')\n",
    "# state_dict = load_file(\"checkpoints/model_epoch4.safetensors\")\n",
    "state_dit = load_file(path)\n",
    "sae_dim    = state_dict[\"head.2.weight\"].shape[0]\n",
    "_, _, _, _, head_dim, _ = get_hyperparams()\n",
    "\n",
    "model = Query2SAE(head_hidden_dim=head_dim, sae_dim=sae_dim)\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# 2) instantiate GPT-2 tokenizer\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\", padding_side=\"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # gpt2 has no pad token by default\n",
    "\n",
    "# 3) tokenize *your question*, not your SAE features!\n",
    "question = \"What is the capital of the US\"\n",
    "inputs   = tokenizer(\n",
    "    question,\n",
    "    return_tensors=\"pt\",\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=256\n",
    ").to(device)\n",
    "\n",
    "# 4) run through your Query2SAE\n",
    "with torch.no_grad():\n",
    "    pred_sae = model(\n",
    "        inputs[\"input_ids\"], \n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )  # shape (1, sae_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: /Users/pardan/Code/SAE/expectation_model\n",
      "After: /Users/pardan/Code/SAE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Before:\", os.getcwd())\n",
    "os.chdir(\"/Users/pardan/Code/SAE\")\n",
    "print(\"After:\", os.getcwd())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from src.interfaces.lens_backend import Variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate F Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pardan/Code/SAE/sae-env/lib/python3.11/site-packages/sae_lens/sae.py:151: UserWarning: \n",
      "This SAE has non-empty model_from_pretrained_kwargs. \n",
      "For optimal performance, load the model like so:\n",
      "model = HookedSAETransformer.from_pretrained_no_processing(..., **cfg.model_from_pretrained_kwargs)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: 'The quick brown fox jumps over the lazy dog.'\n",
      "Shape of SAE feature vectors: torch.Size([1, 11, 24576])\n",
      "Total features: 24576\n",
      "Number of non-zero features: 523\n",
      "Shape of SAE features for the last token: torch.Size([24576])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer_lens import HookedTransformer\n",
    "from sae_lens import SAE\n",
    "\n",
    "def get_sae_features_from_text(\n",
    "    text: str,\n",
    "    sae: SAE,\n",
    "    model: HookedTransformer\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Feeds a text string through a language model, extracts the activations\n",
    "    at the hook point where the SAE was trained, and then encodes these\n",
    "    activations into SAE feature vectors.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be processed.\n",
    "        sae (SAE): The loaded SAE object.\n",
    "        model (HookedTransformer): The loaded TransformerLens model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (1, sequence_length, d_sae)\n",
    "                      containing the SAE's feature activations.\n",
    "    \"\"\"\n",
    "    # 1. Tokenize the text string\n",
    "    # We use prepend_bos=True to add the beginning-of-sequence token\n",
    "    tokens = model.to_tokens(text, prepend_bos=True)\n",
    "    \n",
    "    # 2. Run the model and cache the activations\n",
    "    # We specify the exact hook point where the SAE was trained\n",
    "    hook_name = sae.cfg.hook_name\n",
    "    with torch.no_grad():\n",
    "        _, cache = model.run_with_cache(tokens)\n",
    "        \n",
    "    # 3. Extract the activations from the cache\n",
    "    # The shape will be (batch_size=1, seq_len, d_model=768)\n",
    "    activations = cache[hook_name]\n",
    "\n",
    "    # 4. Use the SAE's encode() method to get the sparse feature vector\n",
    "    # The SAE's input must be flattened to (num_activations, d_model)\n",
    "    # The output is (num_activations, d_sae=24576)\n",
    "    sae_feature_activations = sae.encode(activations)\n",
    "    \n",
    "    return sae_feature_activations\n",
    "\n",
    "# --- Main Usage Example ---\n",
    "\n",
    "# Set up the device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load the models\n",
    "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)\n",
    "sae, _, _ = SAE.from_pretrained(\n",
    "    release=\"gpt2-small-res-jb\",\n",
    "    sae_id=\"blocks.11.hook_resid_pre\",\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Define the input text\n",
    "input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Call the function to get the SAE feature vectors\n",
    "sae_features = get_sae_features_from_text(input_text, sae, model)\n",
    "\n",
    "# Print the shape and some details of the output\n",
    "print(f\"Input text: '{input_text}'\")\n",
    "print(f\"Shape of SAE feature vectors: {sae_features.shape}\")\n",
    "print(f\"Total features: {sae_features.shape[-1]}\")\n",
    "print(f\"Number of non-zero features: {torch.count_nonzero(sae_features).item()}\")\n",
    "\n",
    "# You can now analyze `sae_features`. For example, to see the features\n",
    "# activated for the last token ('dog'):\n",
    "last_token_features = sae_features[0, -1, :]\n",
    "print(f\"Shape of SAE features for the last token: {last_token_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 24576])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg = torch.sum(sae_features, dim=1)\n",
    "agg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_exp = pred_sae.detach()\n",
    "F_act = agg.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.reshape(F_exp, (1,-1))\n",
    "b = np.reshape(F_act, (1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "ans = cosine_similarity(X=a, Y=b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00720794]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_val, exp_idx = torch.topk(F_exp, 500)\n",
    "act_val, act_idx = torch.topk(F_act, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mask = np.isin(exp_val, act_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
