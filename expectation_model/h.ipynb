{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from datasets import load_from_disk, concatenate_datasets, Features, Value, Array2D\n",
    "from huggingface_hub import HfApi\n",
    "import numpy as np\n",
    "import logging\n",
    "from tqdm.auto import tqdm # Use tqdm.auto for notebook compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for clarity\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Configuration ---\n",
    "LOCAL_CHUNKS_DIR = \"temp_processed\" # Directory where your chunks are saved\n",
    "HF_REPO_ID = \"mksethi/gpt2_eli5_sae_features\" # Replace with your Hugging Face username and desired dataset name\n",
    "# Example: \"yourusername/kilt_eli5_sae_features_gpt2_res_jb\"\n",
    "\n",
    "# Define the expected features. This is crucial for consistency.\n",
    "# Ensure these match the dtypes and shapes you saved in generate.py\n",
    "# For gpt2-small-res-jb, the resid_pre dim is 768.\n",
    "# The SAE latent dimension (d_sae) for this specific SAE is 24576.\n",
    "SAE_FEATURES_DIM = 24576 # Corrected based on confirmation for gpt2-small-res-jb SAEs\n",
    "\n",
    "expected_features = Features({\n",
    "    \"input_ids\": Array2D(shape=(256,), dtype=\"int32\"), # CONTEXT_SIZE\n",
    "    \"attention_mask\": Array2D(shape=(256,), dtype=\"int32\"), # CONTEXT_SIZE\n",
    "    \"sae_features\": Array2D(shape=(SAE_FEATURES_DIM,), dtype=\"float32\")\n",
    "})\n",
    "\n",
    "# ... rest of the aggregation and push code ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_and_push_chunks(local_dir, repo_id, features):\n",
    "    \"\"\"\n",
    "    Aggregates data chunks from a local directory and pushes them to Hugging Face Hub.\n",
    "    Includes enhanced logging for better progress visibility.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Starting aggregation process for chunks in: {local_dir}\")\n",
    "\n",
    "    # 1. Discover all chunk directories\n",
    "    chunk_paths = sorted(glob.glob(os.path.join(local_dir, \"rank_*\")))\n",
    "    \n",
    "    if not chunk_paths:\n",
    "        logger.warning(f\"No chunks found in {local_dir}. Please ensure generate.py has run and created files.\")\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Found {len(chunk_paths)} potential chunk directories.\")\n",
    "    \n",
    "    all_datasets = []\n",
    "    \n",
    "    # Use tqdm to show progress of loading chunks\n",
    "    for i, path in enumerate(tqdm(chunk_paths, desc=\"Loading chunks\")):\n",
    "        try:\n",
    "            # Load each chunk as a Dataset\n",
    "            dataset_chunk = load_from_disk(path)\n",
    "            # Optional: Cast here if you suspect schema inconsistencies between chunks\n",
    "            # dataset_chunk = dataset_chunk.cast(features)\n",
    "            all_datasets.append(dataset_chunk)\n",
    "            # logger.debug(f\"Successfully loaded chunk from {path} with {len(dataset_chunk)} examples.\") # Use debug for per-chunk if too verbose\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading chunk from {path}: {e}. Skipping this chunk.\", exc_info=True)\n",
    "            continue\n",
    "\n",
    "    if not all_datasets:\n",
    "        logger.error(\"No valid datasets were loaded. Aborting push to Hugging Face.\")\n",
    "        return\n",
    "\n",
    "    # 2. Concatenate all datasets\n",
    "    logger.info(f\"Concatenating {len(all_datasets)} datasets. This may take a while for large datasets...\")\n",
    "    try:\n",
    "        final_dataset = concatenate_datasets(all_datasets)\n",
    "        logger.info(f\"Final aggregated dataset created with {len(final_dataset)} examples.\")\n",
    "        \n",
    "        # Add a log for the estimated size\n",
    "        # This is a rough estimate but can give a clue.\n",
    "        # Bytes per example (rough guess, depends on content)\n",
    "        # Assuming ~500-1000 bytes per example for input_ids, attn_mask, and 24576 float32s (24576*4 bytes = ~96KB)\n",
    "        # So, it's dominated by sae_features. Let's estimate 100KB per example.\n",
    "        estimated_size_bytes = len(final_dataset) * (SAE_FEATURES_DIM * 4 + 256 * 4 * 2) # sae_features + input_ids + attention_mask\n",
    "        estimated_size_gb = estimated_size_bytes / (1024**3)\n",
    "        logger.info(f\"Estimated raw dataset size: {estimated_size_gb:.2f} GB (before compression).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error concatenating datasets: {e}. Aborting push.\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # 3. Push to Hugging Face Hub\n",
    "    logger.info(f\"Attempting to push aggregated dataset to Hugging Face Hub: {repo_id}\")\n",
    "    \n",
    "# use env variable to get token\n",
    "    if token is None:\n",
    "        logger.warning(\"Hugging Face token not found. Please log in using `huggingface-cli login` in your terminal or set HF_TOKEN environment variable.\")\n",
    "        logger.warning(\"You might not be able to push to the Hub without a valid write token.\")\n",
    "        # We'll let the push_to_hub method raise the authentication error if it fails.\n",
    "\n",
    "    api = HfApi()\n",
    "\n",
    "    try:\n",
    "        logger.info(f\"Checking if repository '{repo_id}' exists and creating if necessary (private=False).\")\n",
    "        api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True, private=False) # Set private=True if desired\n",
    "        logger.info(f\"Repository {repo_id} ensured to exist. Starting data push...\")\n",
    "        \n",
    "        # Push the dataset with a more visible progress bar\n",
    "        # datasets.push_to_hub() has its own progress bar, but we can add a log before/after\n",
    "        final_dataset.push_to_hub(repo_id, private=False) # show_progress=True is key\n",
    "        \n",
    "        logger.info(f\"Successfully pushed dataset to https://huggingface.co/datasets/{repo_id}\")\n",
    "        logger.info(\"Push complete! Check your Hugging Face profile for the dataset.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error pushing dataset to Hugging Face Hub: {e}\", exc_info=True)\n",
    "        logger.error(\"Please ensure you have authenticated with `huggingface-cli login` or set a HF_TOKEN environment variable with write access to the namespace.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_and_push_chunks(\"temp_processed\", \"mksethi/gpt2_eli5_sae_features\", expected_features)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
