{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# load your MLP model\n",
    "model = AutoModel.from_pretrained(\"mksethi/gpt2-query2sae\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the stock GPT‑2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# now you can use tokenizer and model together\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "inputs = tokenizer(\"What is the weather like in hong kong?\", padding='max_length', max_length=256, return_tensors=\"pt\")\n",
    "# len(inputs[\"input_ids\"][0])\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24576"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = outputs[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "values, indices = torch.topk(outputs[0], 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1218, 0.1194, 0.1148, 0.1134, 0.1131, 0.1088, 0.1079, 0.1077, 0.1060,\n",
       "        0.1058, 0.1054, 0.1050, 0.1049, 0.1045, 0.1044, 0.1044, 0.1044, 0.1043,\n",
       "        0.1035, 0.1034, 0.1029, 0.1028, 0.1028, 0.1028, 0.1026, 0.1025, 0.1024,\n",
       "        0.1017, 0.1016, 0.1011, 0.1008, 0.1006, 0.1006, 0.1003, 0.0999, 0.0999,\n",
       "        0.0996, 0.0993, 0.0992, 0.0991, 0.0990, 0.0989, 0.0987, 0.0986, 0.0984,\n",
       "        0.0984, 0.0983, 0.0982, 0.0982, 0.0981])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0512)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.std(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors.torch import save_file\n",
    "import torch\n",
    "state_dict = torch.load(\"./expectation_model/checkpoint/new_model.pt\", map_location=\"cpu\")\n",
    "save_file(state_dict, \"nmodel.safetensors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight → mean: -0.0021, std: 0.0359\n",
      "fc1.bias   → mean: -0.0048, std: 0.0385\n",
      "fc2.weight → mean: 0.0000, std: 0.0509\n",
      "fc2.bias   → mean: 0.0027, std: 0.0512\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# assume `model` is your FrozenLMMLP instance already on CPU or GPU\n",
    "# make sure model is on CPU for easy inspection\n",
    "model_cpu = model.to('cpu')\n",
    "\n",
    "# extract the weight and bias tensors\n",
    "w1 = model_cpu.fc1.weight.data   # shape: (hidden_dim, input_dim)\n",
    "b1 = model_cpu.fc1.bias.data     # shape: (hidden_dim,)\n",
    "\n",
    "# compute mean and std\n",
    "w1_mean, w1_std = w1.mean().item(), w1.std().item()\n",
    "b1_mean, b1_std = b1.mean().item(), b1.std().item()\n",
    "\n",
    "print(f\"fc1.weight → mean: {w1_mean:.4f}, std: {w1_std:.4f}\")\n",
    "print(f\"fc1.bias   → mean: {b1_mean:.4f}, std: {b1_std:.4f}\")\n",
    "\n",
    "# likewise for the second layer:\n",
    "w2 = model_cpu.fc2.weight.data\n",
    "b2 = model_cpu.fc2.bias.data\n",
    "\n",
    "w2_mean, w2_std = w2.mean().item(), w2.std().item()\n",
    "b2_mean, b2_std = b2.mean().item(), b2.std().item()\n",
    "\n",
    "print(f\"fc2.weight → mean: {w2_mean:.4f}, std: {w2_std:.4f}\")\n",
    "print(f\"fc2.bias   → mean: {b2_mean:.4f}, std: {b2_std:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.weight                     → mean: -0.0021, std: 0.0359\n",
      "fc1.bias                       → mean: -0.0048, std: 0.0385\n",
      "fc2.weight                     → mean: 0.0000, std: 0.0509\n",
      "fc2.bias                       → mean: 0.0027, std: 0.0512\n"
     ]
    }
   ],
   "source": [
    "# First, install the safetensors package if you haven’t already:\n",
    "#    pip install safetensors\n",
    "\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# 1. Load the safetensors checkpoint into a state dict\n",
    "state_dict = load_file(\"data.safetensors\")\n",
    "\n",
    "# 2. Filter out the keys for each layer\n",
    "#    (Adjust these prefixes if your actual layers are named differently.)\n",
    "layer_prefixes = [\"fc1.weight\", \"fc1.bias\", \"fc2.weight\", \"fc2.bias\"]\n",
    "\n",
    "for name in layer_prefixes:\n",
    "    # We need to find keys in the state dict that end with our layer name\n",
    "    matching_keys = [k for k in state_dict if k.endswith(name)]\n",
    "    if not matching_keys:\n",
    "        print(f\"No parameters found for layer `{name}`.\")\n",
    "        continue\n",
    "\n",
    "    for key in matching_keys:\n",
    "        tensor = state_dict[key]\n",
    "        # Ensure we have a torch.Tensor\n",
    "        if not isinstance(tensor, torch.Tensor):\n",
    "            tensor = torch.tensor(tensor)\n",
    "\n",
    "        m = tensor.mean().item()\n",
    "        s = tensor.std().item()\n",
    "        print(f\"{key:30s} → mean: {m:.4f}, std: {s:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions from trained weights: input_dim=256, hidden_dim=128, output_dim=24576\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "state_dict = load_file(\"data.safetensors\") # Assuming this is your trained file\n",
    "\n",
    "# Infer input_dim and output_dim from the loaded weights\n",
    "fc1_weight_shape = state_dict['fc1.weight'].shape\n",
    "fc2_weight_shape = state_dict['fc2.weight'].shape\n",
    "\n",
    "trained_input_dim = fc1_weight_shape[1] # input_dim is the second dim of fc1.weight\n",
    "trained_hidden_dim = fc1_weight_shape[0] # hidden_dim is the first dim of fc1.weight\n",
    "trained_output_dim = fc2_weight_shape[0] # output_dim is the first dim of fc2.weight\n",
    "\n",
    "print(f\"Dimensions from trained weights: input_dim={trained_input_dim}, hidden_dim={trained_hidden_dim}, output_dim={trained_output_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sae-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
